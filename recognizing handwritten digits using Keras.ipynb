{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this notebook we will implement Neural network for classifying digits using Keras\n",
    "Also I will show you how to improve model using different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network and trainig\n",
    "NB_EPOCH = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD() # Stochastic Gradient Descent\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# normalize input features\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "[[ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print(Y_train)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need to select the optimizer that is the specific algorithm used to update weights while we train\n",
    "our model\n",
    "We need to select the objective function that is used by the optimizer to navigate the space of\n",
    "weights (frequently, objective functions are called loss function, and the process of optimization is\n",
    "defined as a process of loss minimization)\n",
    "We need to evaluate the trained model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Some common choices for metrics (a complete list of Keras metrics is at https://keras.io/metrics/) are as\n",
    "follows:\n",
    "Accuracy: This is the proportion of correct predictions with respect to the targets\n",
    "\n",
    "Precision: This denotes how many selected items are relevant for a multilabel classification\n",
    "\n",
    "Recall: This denotes how many selected items are relevant for a multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 1.3633 - acc: 0.6796 - val_loss: 0.8904 - val_acc: 0.8246\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.7913 - acc: 0.8272 - val_loss: 0.6572 - val_acc: 0.8546\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.6436 - acc: 0.8497 - val_loss: 0.5625 - val_acc: 0.8681\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.5717 - acc: 0.8602 - val_loss: 0.5098 - val_acc: 0.8765\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.5276 - acc: 0.8678 - val_loss: 0.4758 - val_acc: 0.8826\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.4973 - acc: 0.8726 - val_loss: 0.4515 - val_acc: 0.8866\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.4748 - acc: 0.8775 - val_loss: 0.4333 - val_acc: 0.8882\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.4574 - acc: 0.8803 - val_loss: 0.4189 - val_acc: 0.8920\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.4433 - acc: 0.8834 - val_loss: 0.4075 - val_acc: 0.8939\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.4317 - acc: 0.8850 - val_loss: 0.3977 - val_acc: 0.8966\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.4218 - acc: 0.8873 - val_loss: 0.3896 - val_acc: 0.8984\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.4134 - acc: 0.8888 - val_loss: 0.3827 - val_acc: 0.8995\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.4060 - acc: 0.8902 - val_loss: 0.3766 - val_acc: 0.9003\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3995 - acc: 0.8918 - val_loss: 0.3712 - val_acc: 0.9013\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.3936 - acc: 0.8928 - val_loss: 0.3664 - val_acc: 0.9016\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.3884 - acc: 0.8945 - val_loss: 0.3621 - val_acc: 0.9031\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3837 - acc: 0.8950 - val_loss: 0.3582 - val_acc: 0.9033\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.3794 - acc: 0.8962 - val_loss: 0.3546 - val_acc: 0.9039\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.3755 - acc: 0.8970 - val_loss: 0.3514 - val_acc: 0.9048\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3718 - acc: 0.8979 - val_loss: 0.3485 - val_acc: 0.9053\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3685 - acc: 0.8985 - val_loss: 0.3457 - val_acc: 0.9058\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3653 - acc: 0.8995 - val_loss: 0.3431 - val_acc: 0.9058\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3625 - acc: 0.8999 - val_loss: 0.3407 - val_acc: 0.9063\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3598 - acc: 0.9008 - val_loss: 0.3385 - val_acc: 0.9070\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3572 - acc: 0.9012 - val_loss: 0.3364 - val_acc: 0.9074\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3548 - acc: 0.9019 - val_loss: 0.3345 - val_acc: 0.9084\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.3525 - acc: 0.9022 - val_loss: 0.3326 - val_acc: 0.9082\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.3504 - acc: 0.9032 - val_loss: 0.3311 - val_acc: 0.9090\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.3484 - acc: 0.9031 - val_loss: 0.3293 - val_acc: 0.9094\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.3465 - acc: 0.9041 - val_loss: 0.3277 - val_acc: 0.9097\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.3447 - acc: 0.9044 - val_loss: 0.3264 - val_acc: 0.9097\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3430 - acc: 0.9047 - val_loss: 0.3249 - val_acc: 0.9097\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.3413 - acc: 0.9051 - val_loss: 0.3235 - val_acc: 0.9103\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3397 - acc: 0.9056 - val_loss: 0.3222 - val_acc: 0.9104\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3382 - acc: 0.9058 - val_loss: 0.3211 - val_acc: 0.9110\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3368 - acc: 0.9062 - val_loss: 0.3198 - val_acc: 0.9110\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.3353 - acc: 0.9069 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3340 - acc: 0.9075 - val_loss: 0.3177 - val_acc: 0.9120\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3327 - acc: 0.9075 - val_loss: 0.3166 - val_acc: 0.9122\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.3314 - acc: 0.9078 - val_loss: 0.3159 - val_acc: 0.9118\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.3303 - acc: 0.9080 - val_loss: 0.3147 - val_acc: 0.9127\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.3291 - acc: 0.9084 - val_loss: 0.3138 - val_acc: 0.9132\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.3280 - acc: 0.9089 - val_loss: 0.3130 - val_acc: 0.9132\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3270 - acc: 0.9091 - val_loss: 0.3121 - val_acc: 0.9132\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.3259 - acc: 0.9093 - val_loss: 0.3113 - val_acc: 0.9135\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.3249 - acc: 0.9095 - val_loss: 0.3105 - val_acc: 0.9137\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.3239 - acc: 0.9105 - val_loss: 0.3098 - val_acc: 0.9141\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3230 - acc: 0.9105 - val_loss: 0.3090 - val_acc: 0.9146\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.3221 - acc: 0.9102 - val_loss: 0.3083 - val_acc: 0.9151\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.3212 - acc: 0.9109 - val_loss: 0.3075 - val_acc: 0.9150\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "Test score: 0.306963601744\n",
      "Test accuracy: 0.9151\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################################################################################\n",
    "# first neural network in Keras completed #\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Improving the simple net in Keras with hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 1.4829 - acc: 0.6229 - val_loss: 0.7583 - val_acc: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.6049 - acc: 0.8462 - val_loss: 0.4550 - val_acc: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4398 - acc: 0.8801 - val_loss: 0.3709 - val_acc: 0.9020\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.3767 - acc: 0.8953 - val_loss: 0.3321 - val_acc: 0.9081\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.3415 - acc: 0.9027 - val_loss: 0.3055 - val_acc: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.3175 - acc: 0.9085 - val_loss: 0.2880 - val_acc: 0.9185\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.2989 - acc: 0.9136 - val_loss: 0.2727 - val_acc: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.2839 - acc: 0.9180 - val_loss: 0.2607 - val_acc: 0.9265\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.2714 - acc: 0.9217 - val_loss: 0.2504 - val_acc: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.2602 - acc: 0.9253 - val_loss: 0.2430 - val_acc: 0.9309\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.2501 - acc: 0.9285 - val_loss: 0.2341 - val_acc: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2409 - acc: 0.9301 - val_loss: 0.2271 - val_acc: 0.9353\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2325 - acc: 0.9335 - val_loss: 0.2227 - val_acc: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2253 - acc: 0.9354 - val_loss: 0.2147 - val_acc: 0.9394\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2181 - acc: 0.9374 - val_loss: 0.2082 - val_acc: 0.9413\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2116 - acc: 0.9393 - val_loss: 0.2030 - val_acc: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2055 - acc: 0.9413 - val_loss: 0.1981 - val_acc: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1996 - acc: 0.9430 - val_loss: 0.1932 - val_acc: 0.9459\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1942 - acc: 0.9432 - val_loss: 0.1894 - val_acc: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1890 - acc: 0.9456 - val_loss: 0.1850 - val_acc: 0.9497\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "Test score: 0.186027193424\n",
      "Test accuracy: 0.9463\n"
     ]
    }
   ],
   "source": [
    "# Entire model is same. Only hidden layer has been increased.\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#####################################################################################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Further improving the simple net in Keras with dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/25\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 1.7403 - acc: 0.4540 - val_loss: 0.9291 - val_acc: 0.8125\n",
      "Epoch 2/25\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.9231 - acc: 0.7228 - val_loss: 0.5400 - val_acc: 0.8653\n",
      "Epoch 3/25\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.6935 - acc: 0.7880 - val_loss: 0.4298 - val_acc: 0.8884\n",
      "Epoch 4/25\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.5947 - acc: 0.8208 - val_loss: 0.3790 - val_acc: 0.8977\n",
      "Epoch 5/25\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.5347 - acc: 0.8393 - val_loss: 0.3456 - val_acc: 0.9039\n",
      "Epoch 6/25\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.4976 - acc: 0.8525 - val_loss: 0.3232 - val_acc: 0.9106\n",
      "Epoch 7/25\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.4616 - acc: 0.8629 - val_loss: 0.3048 - val_acc: 0.9130\n",
      "Epoch 8/25\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.4386 - acc: 0.8687 - val_loss: 0.2896 - val_acc: 0.9173\n",
      "Epoch 9/25\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.4181 - acc: 0.8761 - val_loss: 0.2776 - val_acc: 0.9197\n",
      "Epoch 10/25\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.3990 - acc: 0.8837 - val_loss: 0.2657 - val_acc: 0.9233\n",
      "Epoch 11/25\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.3819 - acc: 0.8875 - val_loss: 0.2552 - val_acc: 0.9257\n",
      "Epoch 12/25\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.3688 - acc: 0.8920 - val_loss: 0.2466 - val_acc: 0.9281\n",
      "Epoch 13/25\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.3571 - acc: 0.8944 - val_loss: 0.2388 - val_acc: 0.9300\n",
      "Epoch 14/25\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.3466 - acc: 0.8993 - val_loss: 0.2319 - val_acc: 0.9322\n",
      "Epoch 15/25\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.3359 - acc: 0.9015 - val_loss: 0.2261 - val_acc: 0.9340\n",
      "Epoch 16/25\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.3244 - acc: 0.9056 - val_loss: 0.2180 - val_acc: 0.9353\n",
      "Epoch 17/25\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.3142 - acc: 0.9085 - val_loss: 0.2121 - val_acc: 0.9376\n",
      "Epoch 18/25\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.3102 - acc: 0.9094 - val_loss: 0.2075 - val_acc: 0.9390\n",
      "Epoch 19/25\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.3018 - acc: 0.9118 - val_loss: 0.2018 - val_acc: 0.9409\n",
      "Epoch 20/25\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.2931 - acc: 0.9132 - val_loss: 0.1974 - val_acc: 0.9422\n",
      "Epoch 21/25\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.2866 - acc: 0.9171 - val_loss: 0.1920 - val_acc: 0.9437\n",
      "Epoch 22/25\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2789 - acc: 0.9172 - val_loss: 0.1879 - val_acc: 0.9445\n",
      "Epoch 23/25\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2730 - acc: 0.9199 - val_loss: 0.1841 - val_acc: 0.9463\n",
      "Epoch 24/25\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2686 - acc: 0.9210 - val_loss: 0.1811 - val_acc: 0.9465\n",
      "Epoch 25/25\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2618 - acc: 0.9235 - val_loss: 0.1770 - val_acc: 0.9477\n",
      "10000/10000 [==============================] - 1s 90us/step\n",
      "Test score: 0.178886491237\n",
      "Test accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 25\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################\n",
    "\n",
    "Different methods to improve model\n",
    "\n",
    "Try different hyperparameter tuning \n",
    "- OPTIMIZER = RMSprop()\n",
    "- OPTIMIZER = Adam() \n",
    "- Different dropout values\n",
    "- Increasing the number of epochs\n",
    "- Changing learning rate\n",
    "- Increasing the number of hidden units\n",
    "- Changing mini-batch size - try multiple of 2 - 64, 128, 256, ...\n",
    "\n",
    "Tried different models. Here is the summary\n",
    "\n",
    "    Model/Accuracy   Training  Validation  Test\n",
    "    Simple           92.36%    92.37%      92.22%\n",
    "    Two hidden (128) 94.50%    94.63%      94.41%\n",
    "    Dropout (30%)    98.10%    97.73%      97.7% (200 epochs)\n",
    "    RMSprop          97.97%    97.59%      97.84% (20 epochs)\n",
    "    Adam             98.28%    98.03%      97.93% (20 epochs)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
